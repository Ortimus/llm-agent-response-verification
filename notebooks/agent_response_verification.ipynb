{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1962fc7a",
   "metadata": {},
   "source": [
    "# Don't Trust – Verify: LLM Agent Verification (OpenAI + Groq)\n",
    "\n",
    "This notebook demonstrates an end-to-end verification workflow with:\n",
    "- **Schema validation (Pydantic)** - Ensures structured output\n",
    "- **Evaluator-driven refinement loops** - Checks factual accuracy\n",
    "- **Safety checks** - Toxicity and bias detection\n",
    "- **Controlled simulation** - Demonstrates failure scenarios without API calls\n",
    "- **Multi-provider support** - OpenAI (recommended) or Groq\n",
    "\n",
    "## API Provider Options\n",
    "\n",
    "Goes through .env file and looks for OPENAI_API_KEY, GROQ_API_KEY in order.\n",
    "If no keys are found falls back to use simulation data.\n",
    "\n",
    "1. **OpenAI** (Recommended) - Most stable, widely supported\n",
    "   - Set: `OPENAI_API_KEY` environment variable\n",
    "   - Models: `gpt-4o-mini`, `gpt-4o`, `gpt-3.5-turbo`\n",
    "\n",
    "2. **Groq** (Fast inference) - Good for testing, can be network-sensitive\n",
    "   - Set: `GROQ_API_KEY` environment variable  \n",
    "   - Models: `qwen/qwen3-32b`, `mixtral-8x7b-32768`, `llama-3.1-8b-instant`\n",
    "\n",
    "3. **Simulation** (No API) - Perfect for demos and learning\n",
    "   - No setup needed\n",
    "   - Deterministic scenarios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc04fca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 USING OPENAI: gpt-4o-mini\n",
      "\n",
      "Provider: openai\n",
      "Model: gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "# Setup & imports\n",
    "import os\n",
    "import json\n",
    "from typing import Optional, Literal, Union\n",
    "from enum import Enum\n",
    "from pydantic import BaseModel, Field, ValidationError, field_validator\n",
    "\n",
    "# Try to import API clients\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "    OPENAI_AVAILABLE = True\n",
    "except ImportError:\n",
    "    OPENAI_AVAILABLE = False\n",
    "    print(\"⚠️  OpenAI not installed. Run: pip install openai\")\n",
    "\n",
    "try:\n",
    "    from groq import Groq\n",
    "    GROQ_AVAILABLE = True\n",
    "except ImportError:\n",
    "    GROQ_AVAILABLE = False\n",
    "    print(\"⚠️  Groq not installed. Run: pip install groq\")\n",
    "\n",
    "# Configuration\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# Determine which provider to use\n",
    "if OPENAI_API_KEY and OPENAI_AVAILABLE:\n",
    "    PROVIDER = \"openai\"\n",
    "    MODEL_NAME = \"gpt-4o-mini\"  # Fast, cheap, reliable\n",
    "    print(\"🚀 USING OPENAI: \" + MODEL_NAME)\n",
    "elif GROQ_API_KEY and GROQ_AVAILABLE:\n",
    "    PROVIDER = \"groq\"\n",
    "    MODEL_NAME = \"llama-3.1-8b-instant\"  # Fast Groq model\n",
    "    print(\"🚀 USING GROQ: \" + MODEL_NAME)\n",
    "else:\n",
    "    PROVIDER = \"simulation\"\n",
    "    MODEL_NAME = \"simulation\"\n",
    "    print(\"🎭 SIMULATION MODE: No API keys found\")\n",
    "    print(\"   Set OPENAI_API_KEY or GROQ_API_KEY to use real APIs\")\n",
    "\n",
    "print(f\"\\nProvider: {PROVIDER}\")\n",
    "print(f\"Model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48d93ac",
   "metadata": {},
   "source": [
    "## Test Data: Financial News Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48138ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article to analyze:\n",
      "Rivertown Widgets reported Q3 results beating analyst expectations. \n",
      "Revenue grew 12% year-over-year to $240M, driven by increased demand in the consumer segment. \n",
      "Gross margin improved to 38% from 35% due to operational efficiencies. \n",
      "Management announced a $50M share buyback and reaffirmed full-year guidance. \n",
      "However, supply chain constraints continue to pressure lead times for new product launches.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ARTICLE_TEXT = '''Rivertown Widgets reported Q3 results beating analyst expectations. \n",
    "Revenue grew 12% year-over-year to $240M, driven by increased demand in the consumer segment. \n",
    "Gross margin improved to 38% from 35% due to operational efficiencies. \n",
    "Management announced a $50M share buyback and reaffirmed full-year guidance. \n",
    "However, supply chain constraints continue to pressure lead times for new product launches.\n",
    "'''\n",
    "\n",
    "print(\"Article to analyze:\")\n",
    "print(ARTICLE_TEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34c9a04",
   "metadata": {},
   "source": [
    "## Pydantic Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "00f91beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pydantic schemas defined\n"
     ]
    }
   ],
   "source": [
    "class SummaryOutput(BaseModel):\n",
    "    \"\"\"Structured output for financial article summaries.\"\"\"\n",
    "    \n",
    "    title: str = Field(\n",
    "        ..., \n",
    "        min_length=5,\n",
    "        max_length=100,\n",
    "        description=\"Concise title for the summary\"\n",
    "    )\n",
    "    \n",
    "    summary: str = Field(\n",
    "        ..., \n",
    "        min_length=20,\n",
    "        max_length=500,\n",
    "        description=\"Brief summary of key points\"\n",
    "    )\n",
    "    \n",
    "    key_points: list[str] = Field(\n",
    "        ..., \n",
    "        min_length=2,\n",
    "        max_length=6,\n",
    "        description=\"List of 2-6 key takeaways\"\n",
    "    )\n",
    "    \n",
    "    action_items: list[str] = Field(\n",
    "        ..., \n",
    "        min_length=1,\n",
    "        max_length=5,\n",
    "        description=\"Suggested actions or follow-ups\"\n",
    "    )\n",
    "    \n",
    "    @field_validator('key_points', 'action_items')\n",
    "    @classmethod\n",
    "    def validate_list_items(cls, v):\n",
    "        \"\"\"Ensure list items are non-empty strings.\"\"\"\n",
    "        if not all(isinstance(item, str) and item.strip() for item in v):\n",
    "            raise ValueError(\"All list items must be non-empty strings\")\n",
    "        return v\n",
    "\n",
    "\n",
    "class EvaluationResult(BaseModel):\n",
    "    \"\"\"Result from the evaluator check.\"\"\"\n",
    "    verdict: Literal['correct', 'incorrect']\n",
    "    score: float = Field(ge=0.0, le=1.0)\n",
    "    reasoning: str\n",
    "\n",
    "\n",
    "class ToxicityResult(BaseModel):\n",
    "    \"\"\"Result from toxicity check.\"\"\"\n",
    "    is_toxic: bool\n",
    "    score: float = Field(ge=0.0, le=1.0)\n",
    "    reason: str\n",
    "\n",
    "\n",
    "print(\"✅ Pydantic schemas defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simulation_scenarios",
   "metadata": {},
   "source": [
    "## Simulation Engine\n",
    "\n",
    "Controlled test scenarios for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simulation_engine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Simulation engine initialized\n"
     ]
    }
   ],
   "source": [
    "class SimulationScenario(Enum):\n",
    "    \"\"\"Defines different failure scenarios for testing.\"\"\"\n",
    "    INVALID_JSON = \"invalid_json\"\n",
    "    MISSING_FIELD = \"missing_field\"\n",
    "    TOXIC_CONTENT = \"toxic_content\"\n",
    "    FACTUAL_ERROR = \"factual_error\"\n",
    "    SUCCESS = \"success\"\n",
    "\n",
    "\n",
    "class SimulationEngine:\n",
    "    \"\"\"Generates controlled responses for each scenario.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.scenario_cycle = [\n",
    "            SimulationScenario.INVALID_JSON,\n",
    "            SimulationScenario.MISSING_FIELD,\n",
    "            SimulationScenario.TOXIC_CONTENT,\n",
    "            SimulationScenario.FACTUAL_ERROR,\n",
    "            SimulationScenario.SUCCESS\n",
    "        ]\n",
    "        self.current_index = 0\n",
    "    \n",
    "    def get_next_scenario(self) -> SimulationScenario:\n",
    "        \"\"\"Get the next scenario in the cycle.\"\"\"\n",
    "        scenario = self.scenario_cycle[self.current_index]\n",
    "        self.current_index = (self.current_index + 1) % len(self.scenario_cycle)\n",
    "        return scenario\n",
    "    \n",
    "    def generate_response(self, scenario: SimulationScenario) -> str:\n",
    "        \"\"\"Generate a response for the given scenario.\"\"\"\n",
    "        \n",
    "        if scenario == SimulationScenario.INVALID_JSON:\n",
    "            return '''{\n",
    "                \"title\": \"Rivertown Q3 Results\",\n",
    "                \"summary\": \"Strong quarter with revenue growth\"\n",
    "                // Missing closing brace and other fields\n",
    "            '''\n",
    "        \n",
    "        elif scenario == SimulationScenario.MISSING_FIELD:\n",
    "            return json.dumps({\n",
    "                \"title\": \"Rivertown Q3 Results\",\n",
    "                \"summary\": \"Rivertown Widgets reported strong Q3 results.\",\n",
    "                \"key_points\": [\"Revenue growth\", \"Margin improvement\"]\n",
    "            })\n",
    "        \n",
    "        elif scenario == SimulationScenario.TOXIC_CONTENT:\n",
    "            return json.dumps({\n",
    "                \"title\": \"Rivertown's Garbage Quarter\",\n",
    "                \"summary\": \"This report is trash and management are idiots for focusing on consumer instead of enterprise.\",\n",
    "                \"key_points\": [\n",
    "                    \"Stupid revenue decisions\",\n",
    "                    \"Incompetent supply chain management\"\n",
    "                ],\n",
    "                \"action_items\": [\"Fire the management team\"]\n",
    "            })\n",
    "        \n",
    "        elif scenario == SimulationScenario.FACTUAL_ERROR:\n",
    "            return json.dumps({\n",
    "                \"title\": \"Rivertown Q3 Results\",\n",
    "                \"summary\": \"Rivertown beat expectations with 18% YoY revenue growth to $300M, driven by enterprise sales expansion.\",\n",
    "                \"key_points\": [\n",
    "                    \"Revenue +18% YoY to $300M\",\n",
    "                    \"Enterprise segment drove growth\",\n",
    "                    \"Margins improved to 42%\"\n",
    "                ],\n",
    "                \"action_items\": [\n",
    "                    \"Review enterprise customer contracts\",\n",
    "                    \"Analyze cost reduction initiatives\"\n",
    "                ]\n",
    "            })\n",
    "        \n",
    "        else:  # SUCCESS\n",
    "            return json.dumps({\n",
    "                \"title\": \"Rivertown Widgets Beats Q3 Expectations\",\n",
    "                \"summary\": \"Rivertown Widgets exceeded analyst expectations in Q3 with 12% YoY revenue growth to $240M, driven by consumer segment demand. Gross margin improved from 35% to 38% through operational efficiencies. Management announced a $50M share buyback while noting ongoing supply chain challenges.\",\n",
    "                \"key_points\": [\n",
    "                    \"Revenue grew 12% YoY to $240M\",\n",
    "                    \"Consumer segment drove growth\",\n",
    "                    \"Gross margin improved to 38% (from 35%)\",\n",
    "                    \"Announced $50M share buyback program\",\n",
    "                    \"Supply chain constraints persist\"\n",
    "                ],\n",
    "                \"action_items\": [\n",
    "                    \"Monitor supply chain developments\",\n",
    "                    \"Track share buyback execution\",\n",
    "                    \"Review full-year guidance updates\"\n",
    "                ]\n",
    "            })\n",
    "    \n",
    "    def evaluate_response(self, response_dict: dict, article: str) -> EvaluationResult:\n",
    "        \"\"\"Simulate evaluator check.\"\"\"\n",
    "        summary = response_dict.get(\"summary\", \"\").lower()\n",
    "        key_points = [kp.lower() for kp in response_dict.get(\"key_points\", [])]\n",
    "        \n",
    "        # Check for factual errors\n",
    "        if \"18%\" in summary or \"$300m\" in summary:\n",
    "            return EvaluationResult(\n",
    "                verdict='incorrect',\n",
    "                score=0.3,\n",
    "                reasoning=\"Revenue figures are incorrect. Article states 12% growth to $240M, not 18% to $300M.\"\n",
    "            )\n",
    "        \n",
    "        if \"enterprise\" in summary or any(\"enterprise\" in kp for kp in key_points):\n",
    "            return EvaluationResult(\n",
    "                verdict='incorrect',\n",
    "                score=0.4,\n",
    "                reasoning=\"Incorrect segment attribution. Article states consumer segment drove growth, not enterprise.\"\n",
    "            )\n",
    "        \n",
    "        if \"42%\" in summary:\n",
    "            return EvaluationResult(\n",
    "                verdict='incorrect',\n",
    "                score=0.5,\n",
    "                reasoning=\"Margin figure is incorrect. Article states 38% gross margin, not 42%.\"\n",
    "            )\n",
    "        \n",
    "        if not response_dict.get(\"action_items\"):\n",
    "            return EvaluationResult(\n",
    "                verdict='incorrect',\n",
    "                score=0.6,\n",
    "                reasoning=\"Missing action items. Summary should include suggested follow-up actions.\"\n",
    "            )\n",
    "        \n",
    "        return EvaluationResult(\n",
    "            verdict='correct',\n",
    "            score=0.92,\n",
    "            reasoning=\"Summary is accurate, complete, and includes all key information from the article.\"\n",
    "        )\n",
    "    \n",
    "    def check_toxicity(self, text: str) -> ToxicityResult:\n",
    "        \n",
    "        \"\"\"Simulate toxicity check.\n",
    "        \n",
    "        Simplified toxicity check for demonstration purposes.\n",
    "        \n",
    "        ⚠️ PRODUCTION NOTE: Use dedicated safety APIs:\n",
    "        - OpenAI Moderation API (free, fast)\n",
    "        - Azure Content Safety\n",
    "        - Perspective API (Google)\n",
    "        - HuggingFace: unitary/toxic-bert\n",
    "        - Detoxify library\n",
    "        \n",
    "        This keyword-based approach demonstrates the verification \n",
    "        pattern without adding heavy dependencies.\n",
    "        \"\"\"\n",
    "        toxic_keywords = ['garbage', 'trash', 'idiots', 'stupid', 'incompetent', 'fire']\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        found_toxic = [word for word in toxic_keywords if word in text_lower]\n",
    "        \n",
    "        if found_toxic:\n",
    "            return ToxicityResult(\n",
    "                is_toxic=True,\n",
    "                score=0.85,\n",
    "                reason=f\"Detected inappropriate language: {', '.join(found_toxic)}\"\n",
    "            )\n",
    "        \n",
    "        return ToxicityResult(\n",
    "            is_toxic=False,\n",
    "            score=0.05,\n",
    "            reason=\"No toxic content detected\"\n",
    "        )\n",
    "\n",
    "\n",
    "sim_engine = SimulationEngine()\n",
    "print(\"✅ Simulation engine initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "llm_interface",
   "metadata": {},
   "source": [
    "## Multi-Provider LLM Interface\n",
    "\n",
    "Supports OpenAI, Groq, and simulation mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "llm_functions",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Multi-provider LLM interface defined\n"
     ]
    }
   ],
   "source": [
    "class LLMClient:\n",
    "    \"\"\"Unified interface for multiple LLM providers.\"\"\"\n",
    "    \n",
    "    def __init__(self, provider: str = PROVIDER, model: str = MODEL_NAME):\n",
    "        self.provider = provider\n",
    "        self.model = model\n",
    "        self.client = None\n",
    "        \n",
    "        if provider == \"openai\":\n",
    "            try:\n",
    "                self.client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "                # Test the connection\n",
    "                self.client.models.list()\n",
    "                print(f\"✅ Connected to OpenAI ({model})\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ OpenAI connection failed: {e}\")\n",
    "                print(\"   Falling back to simulation mode\")\n",
    "                self.provider = \"simulation\"\n",
    "                self.client = None\n",
    "        \n",
    "        elif provider == \"groq\":\n",
    "            try:\n",
    "                self.client = Groq(api_key=GROQ_API_KEY)\n",
    "                print(f\"✅ Connected to Groq ({model})\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Groq connection failed: {e}\")\n",
    "                print(\"   Falling back to simulation mode\")\n",
    "                self.provider = \"simulation\"\n",
    "                self.client = None\n",
    "        \n",
    "        else:\n",
    "            print(\"✅ Using simulation mode\")\n",
    "    \n",
    "    def generate(self, prompt: str, scenario: Optional[SimulationScenario] = None) -> str:\n",
    "        \"\"\"Generate a response using the configured provider.\"\"\"\n",
    "        \n",
    "        if self.provider == \"simulation\":\n",
    "            if scenario is None:\n",
    "                scenario = sim_engine.get_next_scenario()\n",
    "            print(f\"   📋 Simulating: {scenario.value}\")\n",
    "            return sim_engine.generate_response(scenario)\n",
    "        \n",
    "        elif self.provider == \"openai\":\n",
    "            try:\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    temperature=0.7,\n",
    "                    max_tokens=800\n",
    "                )\n",
    "                return response.choices[0].message.content\n",
    "            except Exception as e:\n",
    "                print(f\"❌ OpenAI API error: {e}\")\n",
    "                print(\"   Using simulation fallback\")\n",
    "                return sim_engine.generate_response(SimulationScenario.SUCCESS)\n",
    "        \n",
    "        elif self.provider == \"groq\":\n",
    "            try:\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    temperature=0.7,\n",
    "                    max_tokens=800\n",
    "                )\n",
    "                return response.choices[0].message.content\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Groq API error: {e}\")\n",
    "                print(\"   Using simulation fallback\")\n",
    "                return sim_engine.generate_response(SimulationScenario.SUCCESS)\n",
    "    \n",
    "    def evaluate(self, summary_dict: dict, article: str) -> EvaluationResult:\n",
    "        \"\"\"Evaluate summary accuracy.\"\"\"\n",
    "        \n",
    "        if self.provider == \"simulation\" or self.client is None:\n",
    "            return sim_engine.evaluate_response(summary_dict, article)\n",
    "        \n",
    "        eval_prompt = f\"\"\"You are an evaluator. Compare this summary to the article and judge factual accuracy.\n",
    "\n",
    "Article: {article}\n",
    "\n",
    "Summary: {json.dumps(summary_dict, indent=2)}\n",
    "\n",
    "Return ONLY valid JSON with exactly this structure (no markdown, no extra text):\n",
    "{{\n",
    "  \"verdict\": \"correct\" or \"incorrect\",\n",
    "  \"score\": 0.0 to 1.0,\n",
    "  \"reasoning\": \"explanation\"\n",
    "}}\"\"\"\n",
    "        \n",
    "        try:\n",
    "            raw_response = self.generate(eval_prompt)\n",
    "            \n",
    "            # Clean up response (remove markdown if present)\n",
    "            raw_response = raw_response.strip()\n",
    "            if raw_response.startswith(\"```json\"):\n",
    "                raw_response = raw_response.split(\"```json\")[1].split(\"```\")[0]\n",
    "            elif raw_response.startswith(\"```\"):\n",
    "                raw_response = raw_response.split(\"```\")[1].split(\"```\")[0]\n",
    "            \n",
    "            result = json.loads(raw_response)\n",
    "            return EvaluationResult(**result)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Evaluator parse error: {e}\")\n",
    "            print(\"   Using simulation evaluator\")\n",
    "            return sim_engine.evaluate_response(summary_dict, article)\n",
    "    \n",
    "    def check_toxicity(self, text: str) -> ToxicityResult:\n",
    "        \"\"\"Check for toxic content.\"\"\"\n",
    "        \n",
    "        if self.provider == \"simulation\" or self.client is None:\n",
    "            return sim_engine.check_toxicity(text)\n",
    "        \n",
    "        tox_prompt = f\"\"\"You are a content safety detector. Analyze this text for toxic, inappropriate, or unprofessional language.\n",
    "\n",
    "Text: {text}\n",
    "\n",
    "Return ONLY valid JSON (no markdown):\n",
    "{{\n",
    "  \"is_toxic\": true or false,\n",
    "  \"score\": 0.0 to 1.0,\n",
    "  \"reason\": \"explanation\"\n",
    "}}\"\"\"\n",
    "        \n",
    "        try:\n",
    "            raw_response = self.generate(tox_prompt)\n",
    "            \n",
    "            # Clean up response\n",
    "            raw_response = raw_response.strip()\n",
    "            if raw_response.startswith(\"```json\"):\n",
    "                raw_response = raw_response.split(\"```json\")[1].split(\"```\")[0]\n",
    "            elif raw_response.startswith(\"```\"):\n",
    "                raw_response = raw_response.split(\"```\")[1].split(\"```\")[0]\n",
    "            \n",
    "            result = json.loads(raw_response)\n",
    "            return ToxicityResult(**result)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Toxicity check error: {e}\")\n",
    "            print(\"   Using simulation fallback\")\n",
    "            return sim_engine.check_toxicity(text)\n",
    "\n",
    "\n",
    "print(\"✅ Multi-provider LLM interface defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verification_loop",
   "metadata": {},
   "source": [
    "## Verification Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "verification_loop_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Verification loop function defined\n"
     ]
    }
   ],
   "source": [
    "def verification_loop(\n",
    "    client: LLMClient,\n",
    "    article: str,\n",
    "    prompt: str,\n",
    "    max_rounds: int = 5,\n",
    "    confidence_threshold: float = 0.75,\n",
    "    force_scenarios: Optional[list[SimulationScenario]] = None\n",
    ") -> tuple[Optional[SummaryOutput], Optional[EvaluationResult], bool]:\n",
    "    \"\"\"\n",
    "    Run the verification loop with refinement.\n",
    "    \n",
    "    Returns:\n",
    "        (final_output, evaluation, needs_human_review)\n",
    "    \"\"\"\n",
    "    \n",
    "    current_prompt = prompt\n",
    "    \n",
    "    for round_num in range(1, max_rounds + 1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"🔄 Round {round_num}/{max_rounds}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Determine scenario\n",
    "        scenario = None\n",
    "        if client.provider == \"simulation\" and force_scenarios and round_num <= len(force_scenarios):\n",
    "            scenario = force_scenarios[round_num - 1]\n",
    "        \n",
    "        # STEP 1: Generate\n",
    "        print(\"\\n📝 Step 1: Generating response...\")\n",
    "        full_prompt = f\"{current_prompt}\\n\\nArticle:\\n{article}\\n\\nReturn ONLY valid JSON (no markdown, no extra text) matching this structure:\\n{json.dumps(SummaryOutput.model_json_schema(), indent=2)}\"\n",
    "        \n",
    "        raw_response = client.generate(full_prompt, scenario)\n",
    "        \n",
    "        # Clean markdown wrapper if present\n",
    "        raw_response = raw_response.strip()\n",
    "        if raw_response.startswith(\"```json\"):\n",
    "            raw_response = raw_response.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "        elif raw_response.startswith(\"```\"):\n",
    "            raw_response = raw_response.split(\"```\")[1].split(\"```\")[0].strip()\n",
    "        \n",
    "        print(f\"Raw output ({len(raw_response)} chars):\\n{raw_response[:300]}...\")\n",
    "        \n",
    "        # STEP 2: Parse & Validate\n",
    "        print(\"\\n🔍 Step 2: Validating structure...\")\n",
    "        try:\n",
    "            response_dict = json.loads(raw_response)\n",
    "            summary_obj = SummaryOutput(**response_dict)\n",
    "            print(\"✅ Schema validation passed\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"❌ Invalid JSON: {e}\")\n",
    "            current_prompt = f\"{prompt}\\n\\nPrevious attempt failed JSON parsing. Return ONLY valid JSON with no markdown formatting, comments, or extra text.\"\n",
    "            continue\n",
    "        except ValidationError as e:\n",
    "            print(f\"❌ Schema validation failed:\")\n",
    "            for error in e.errors():\n",
    "                print(f\"   - {error['loc']}: {error['msg']}\")\n",
    "            current_prompt = f\"{prompt}\\n\\nPrevious attempt failed validation: {e}\\nPlease fix these issues and return valid JSON.\"\n",
    "            continue\n",
    "        \n",
    "        # STEP 3: Toxicity Check\n",
    "        print(\"\\n🛡️ Step 3: Safety check...\")\n",
    "        tox_result = client.check_toxicity(summary_obj.summary)\n",
    "        print(f\"Toxicity score: {tox_result.score:.2f}\")\n",
    "        \n",
    "        if tox_result.is_toxic:\n",
    "            print(f\"⚠️ TOXIC CONTENT: {tox_result.reason}\")\n",
    "            current_prompt = f\"{prompt}\\n\\nPrevious attempt contained inappropriate content: {tox_result.reason}\\nProvide a professional, neutral summary.\"\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"✅ Safety check passed: {tox_result.reason}\")\n",
    "        \n",
    "        # STEP 4: Evaluate Accuracy\n",
    "        print(\"\\n📊 Step 4: Evaluating accuracy...\")\n",
    "        eval_result = client.evaluate(response_dict, article)\n",
    "        print(f\"Verdict: {eval_result.verdict.upper()}\")\n",
    "        print(f\"Score: {eval_result.score:.2f}\")\n",
    "        print(f\"Reasoning: {eval_result.reasoning}\")\n",
    "        \n",
    "        # STEP 5: Decision\n",
    "        if eval_result.verdict == 'correct' and eval_result.score >= confidence_threshold:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(\"✅ ACCEPTED: Response meets quality threshold\")\n",
    "            print(f\"{'='*60}\")\n",
    "            return summary_obj, eval_result, False\n",
    "        \n",
    "        # Prepare feedback for next round\n",
    "        print(f\"\\n🔧 REFINEMENT NEEDED: {eval_result.reasoning}\")\n",
    "        current_prompt = f\"{prompt}\\n\\nEvaluator feedback: {eval_result.reasoning}\\nPlease correct these issues.\"\n",
    "    \n",
    "    # Max rounds reached\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"⚠️ MAX ROUNDS REACHED: Escalating to human review\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return (\n",
    "        summary_obj if 'summary_obj' in locals() else None,\n",
    "        eval_result if 'eval_result' in locals() else None,\n",
    "        True\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"✅ Verification loop function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demo_section",
   "metadata": {},
   "source": [
    "## Demo: Run Verification Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize client\n",
    "client = LLMClient()\n",
    "\n",
    "# Agent prompt\n",
    "AGENT_PROMPT = \"\"\"You are a financial analyst. Summarize this earnings report accurately.\n",
    "Focus on key metrics, segment performance, and management guidance.\n",
    "Be factual and professional.\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"🚀 STARTING VERIFICATION DEMO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Define test scenarios (only used in simulation mode)\n",
    "test_scenarios = [\n",
    "    SimulationScenario.INVALID_JSON,\n",
    "    SimulationScenario.MISSING_FIELD,\n",
    "    SimulationScenario.TOXIC_CONTENT,\n",
    "    SimulationScenario.FACTUAL_ERROR,\n",
    "    SimulationScenario.SUCCESS\n",
    "]\n",
    "\n",
    "final_output, final_eval, needs_human = verification_loop(\n",
    "    client=client,\n",
    "    article=ARTICLE_TEXT,\n",
    "    prompt=AGENT_PROMPT,\n",
    "    max_rounds=6,\n",
    "    confidence_threshold=0.75,\n",
    "    force_scenarios=test_scenarios if client.provider == \"simulation\" else None\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"📋 FINAL RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if final_output:\n",
    "    print(\"\\n✅ Final Output:\")\n",
    "    print(json.dumps(final_output.model_dump(), indent=2))\n",
    "\n",
    "if final_eval:\n",
    "    print(f\"\\n📊 Final Evaluation:\")\n",
    "    print(f\"   Verdict: {final_eval.verdict}\")\n",
    "    print(f\"   Score: {final_eval.score:.2f}\")\n",
    "    print(f\"   Reasoning: {final_eval.reasoning}\")\n",
    "\n",
    "print(f\"\\n👤 Needs Human Review: {needs_human}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save_results",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "results_summary = {\n",
    "    \"timestamp\": datetime.datetime.utcnow().isoformat() + \"Z\",\n",
    "    \"provider\": PROVIDER,\n",
    "    \"model\": MODEL_NAME,\n",
    "    \"final_decision\": \"HUMAN_REVIEW\" if needs_human else (\n",
    "        \"APPROVED\" if final_eval and final_eval.verdict == 'correct' else \"REJECTED\"\n",
    "    ),\n",
    "    \"confidence_score\": final_eval.score if final_eval else 0.0,\n",
    "    \"final_output\": final_output.model_dump() if final_output else None,\n",
    "    \"evaluation\": final_eval.model_dump() if final_eval else None\n",
    "}\n",
    "\n",
    "with open(\"verification_results.json\", \"w\") as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(f\"✅ Results saved to verification_results.json\")\n",
    "print(\"\\nSummary:\")\n",
    "print(json.dumps(results_summary, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37b26d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw output (1184 chars):\n",
      "{\n",
      "  \"title\": \"TechFlow Systems Q4 Preliminary Financial Report\",\n",
      "  \"summary\": \"TechFlow Systems has announced its preliminary Q4 results, reporting revenue 'around $200M' with exact figures pending audit. The company described its growth as 'strong', though the percentage growth was not disclosed. L...\n",
      "\n",
      "🔍 Step 2: Validating structure...\n",
      "✅ Schema validation passed\n",
      "\n",
      "🛡️ Step 3: Safety check...\n",
      "Toxicity score: 0.00\n",
      "✅ Safety check passed: The text is professional and contains no toxic, inappropriate, or unprofessional language.\n",
      "\n",
      "📊 Step 4: Evaluating accuracy...\n",
      "Verdict: INCORRECT\n",
      "Score: 0.59\n",
      "Reasoning: Better, but 'significant margin improvements' is not quantified. Must explicitly state 'company did not provide specific percentage'.\n",
      "\n",
      "🔧 REFINEMENT NEEDED: Better, but 'significant margin improvements' is not quantified. Must explicitly state 'company did not provide specific percentage'.\n",
      "\n",
      "============================================================\n",
      "⚠️ MAX ROUNDS REACHED: Escalating to human review\n",
      "============================================================\n",
      "\\n======================================================================\n",
      "📋 RESULTS: HUMAN-IN-THE-LOOP TRIGGERED\n",
      "======================================================================\n",
      "\\n✅ EXPECTED BEHAVIOR: Escalated to human review\n",
      "\\n📊 Final Status:\n",
      "   • Rounds attempted: 3/3\n",
      "   • Best score achieved: 0.59\n",
      "   • Threshold required: 0.75\n",
      "   • Gap: 0.16\n",
      "\\n❓ Last issue: Better, but 'significant margin improvements' is not quantified. Must explicitly state 'company did not provide specific percentage'.\n",
      "\\n👤 Human Review Queue:\n",
      "   1. Review the LLM's best attempt below\n",
      "   2. Assess if threshold is appropriate for this content\n",
      "   3. Manually refine or reject\n",
      "   4. Update prompt templates for similar cases\n",
      "\\n📄 LLM's Best Attempt (for human to review):\n",
      "{\n",
      "  \"title\": \"TechFlow Systems Q4 Preliminary Financial Report\",\n",
      "  \"summary\": \"TechFlow Systems has announced its preliminary Q4 results, reporting revenue 'around $200M' with exact figures pending audit. The company described its growth as 'strong', though the percentage growth was not disclosed. Leadership also indicated 'significant' margin improvements and mentioned they are 'exploring' strategic options. A conference call with analysts is scheduled for next month to provide further details.\",\n",
      "  \"key_points\": [\n",
      "    \"Q4 revenue reported as 'around $200M'\",\n",
      "    \"Exact revenue figures pending audit\",\n",
      "    \"Growth described as 'strong', but percentage not disclosed\",\n",
      "    \"Significant margin improvements mentioned\",\n",
      "    \"Exploring strategic options\",\n",
      "    \"Analyst conference call scheduled for next month\"\n",
      "  ],\n",
      "  \"action_items\": [\n",
      "    \"Monitor the upcoming analyst conference call for detailed breakdowns\",\n",
      "    \"Request clarification on the growth percentage from the company\",\n",
      "    \"Follow up on the audit results for exact revenue figures\",\n",
      "    \"Investigate potential impacts of strategic options being explored\",\n",
      "    \"Analyze margin improvement details when disclosed\"\n",
      "  ]\n",
      "}\n",
      "\\n💡 Production Recommendation:\n",
      "   • Queue in 'Needs Human Review' dashboard\n",
      "   • Priority: HIGH (ambiguous source data)\n",
      "   • Assign to: Senior Analyst\n",
      "   • Context: Article has preliminary/unaudited figures\n",
      "\\n======================================================================\n",
      "✅ HITL Demo Complete\n",
      "======================================================================\n",
      "\\nKey Insight: Better to escalate than accept low-quality output!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Demonstrates Human-in-the-Loop (HITL) Escalation\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*70)\n",
    "print(\"🚨 DEMO: HUMAN-IN-THE-LOOP ESCALATION\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\\\nShowing what happens when verification repeatedly fails...\")\n",
    "\n",
    "# Ambiguous article that's hard to get right\n",
    "AMBIGUOUS_ARTICLE = \"\"\"\n",
    "TechFlow Systems announced Q4 results with revenue \"around $200M\" \n",
    "(exact figures pending audit). Growth was described as \"strong\" but \n",
    "percentage not disclosed. Leadership mentioned \"significant\" margin \n",
    "improvements and \"exploring\" strategic options. Analyst conference \n",
    "call scheduled for next month will provide detailed breakdowns.\n",
    "\"\"\"\n",
    "\n",
    "# Create a custom evaluator that keeps rejecting\n",
    "class PerfectionistEvaluator:\n",
    "    \"\"\"Evaluator that's never satisfied - to demonstrate HITL escalation.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.round = 0\n",
    "    \n",
    "    def evaluate(self, summary_dict, article):\n",
    "        self.round += 1\n",
    "        # Always find something wrong with decreasing scores\n",
    "        reasons = [\n",
    "            \"Vague terms like 'around $200M' need explicit uncertainty markers. Use 'approximately' or 'pending audit' qualifiers.\",\n",
    "            \"Revenue qualified now, but 'strong growth' without percentage is too vague. Must note 'percentage not disclosed by company'.\",\n",
    "            \"Better, but 'significant margin improvements' is not quantified. Must explicitly state 'company did not provide specific percentage'.\",\n",
    "            \"Improved again, but action items must include 'await detailed analyst call next month' given the preliminary nature of data.\",\n",
    "            \"Still missing explicit caveats that all figures are preliminary and subject to audit confirmation.\"\n",
    "        ]\n",
    "        \n",
    "        idx = min(self.round - 1, len(reasons) - 1)\n",
    "        score = max(0.74 - (self.round * 0.05), 0.50)  # Decreasing scores, never reaches 0.75\n",
    "        \n",
    "        return EvaluationResult(\n",
    "            verdict='incorrect',\n",
    "            score=score,\n",
    "            reasoning=reasons[idx]\n",
    "        )\n",
    "\n",
    "# Wrapper client\n",
    "class HITLClient:\n",
    "    def __init__(self, base_client):\n",
    "        self.base_client = base_client\n",
    "        self.provider = base_client.provider\n",
    "        self.model = base_client.model\n",
    "        self.client = base_client.client\n",
    "        self.evaluator = PerfectionistEvaluator()\n",
    "    \n",
    "    def generate(self, prompt, scenario=None):\n",
    "        return self.base_client.generate(prompt, scenario)\n",
    "    \n",
    "    def check_toxicity(self, text):\n",
    "        return self.base_client.check_toxicity(text)\n",
    "    \n",
    "    def evaluate(self, summary_dict, article):\n",
    "        return self.evaluator.evaluate(summary_dict, article)\n",
    "\n",
    "# Run HITL demo\n",
    "hitl_client = HITLClient(client)\n",
    "\n",
    "HITL_PROMPT = \"\"\"Summarize this preliminary financial report accurately.\n",
    "Note all uncertainties and pending disclosures explicitly.\"\"\"\n",
    "\n",
    "print(\"\\\\nRunning with max_rounds=3 and threshold=0.75...\")\n",
    "print(\"(Evaluator will keep rejecting to show HITL escalation)\\\\n\")\n",
    "\n",
    "final_output, final_eval, needs_human = verification_loop(\n",
    "    client=hitl_client,\n",
    "    article=AMBIGUOUS_ARTICLE,\n",
    "    prompt=HITL_PROMPT,\n",
    "    max_rounds=3,\n",
    "    confidence_threshold=0.75\n",
    ")\n",
    "\n",
    "# Display HITL results\n",
    "print(\"\\\\n\" + \"=\"*70)\n",
    "print(\"📋 RESULTS: HUMAN-IN-THE-LOOP TRIGGERED\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if needs_human:\n",
    "    print(\"\\\\n✅ EXPECTED BEHAVIOR: Escalated to human review\")\n",
    "    print(f\"\\\\n📊 Final Status:\")\n",
    "    print(f\"   • Rounds attempted: 3/3\")\n",
    "    print(f\"   • Best score achieved: {final_eval.score:.2f}\")\n",
    "    print(f\"   • Threshold required: 0.75\")\n",
    "    print(f\"   • Gap: {0.75 - final_eval.score:.2f}\")\n",
    "    print(f\"\\\\n❓ Last issue: {final_eval.reasoning}\")\n",
    "    \n",
    "    print(\"\\\\n👤 Human Review Queue:\")\n",
    "    print(\"   1. Review the LLM's best attempt below\")\n",
    "    print(\"   2. Assess if threshold is appropriate for this content\")\n",
    "    print(\"   3. Manually refine or reject\")\n",
    "    print(\"   4. Update prompt templates for similar cases\")\n",
    "    \n",
    "    if final_output:\n",
    "        print(\"\\\\n📄 LLM's Best Attempt (for human to review):\")\n",
    "        print(json.dumps(final_output.model_dump(), indent=2))\n",
    "    \n",
    "    print(\"\\\\n💡 Production Recommendation:\")\n",
    "    print(\"   • Queue in 'Needs Human Review' dashboard\")\n",
    "    print(\"   • Priority: HIGH (ambiguous source data)\")\n",
    "    print(\"   • Assign to: Senior Analyst\")\n",
    "    print(\"   • Context: Article has preliminary/unaudited figures\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*70)\n",
    "print(\"✅ HITL Demo Complete\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\\\nKey Insight: Better to escalate than accept low-quality output!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
